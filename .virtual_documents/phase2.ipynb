





# import the principal libraries
import pandas as pd 
import numpy as np 
# import torch
import os
from groq import Groq, Client
from dotenv import load_dotenv 
load_dotenv()

import dspy
import tqdm as notebook_tqdm
# import typos
from typing import Dict, Tuple, Optional, List, Set, Optional

# import other libraries
from enum import Enum 
from dataclasses import dataclass


open_ai_api_key = os.getenv("OPENAI_API_KEY")


# import keys from .env
load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")
client = Client(
    api_key=groq_api_key
    )
llmo = dspy.LM('openai/gpt-4o-mini', api_key=open_ai_api_key)
dspy.configure(lm=llmo)
# Load the clean document
# doc_path = "cleaned_text.txt"
# with open(doc_path, "r") as f:
#     INPUT_FILE = f.read()


test_file = """The Eagle, the Hare, and the Beetle
A hare was being chased by an eagle, and seeing herself lost, she begged a beetle for help, pleading for its assistance.
The beetle asked the eagle to spare his friend. But the eagle, despising the insignificance of the beetle, devoured the hare in his presence.
From then on, seeking revenge, the beetle observed the places where the eagle laid its eggs, and rolling them, knocked them to the ground. Seeing herself driven away from wherever she went, the eagle turned to Zeus asking for a safe place to lay her eggs.
Zeus offered to let her place them in his lap, but the beetle, seeing this escape tactic, made a small ball of dung, flew up and dropped it on Zeus's lap.
Zeus then stood up to shake off that filth, and unknowingly threw the eggs to the ground. That is why since then, eagles do not lay eggs during the season when beetles come out to fly.
Moral: Never despise what seems insignificant, for there is no being so weak that it cannot reach you."""





class ExtractRelevantInformation(dspy.Signature):
    """Extract the relevant information and key themes about this story"""
    input_text: str = dspy.InputField()
    reasoning: str = dspy.OutputField(desc="Step by step reasoning and extraction of key themes")
    moral_of_the_story: str = dspy.OutputField(desc="The moral lesson or teaching (Ense√±anza) of the story")
    key_themes: str = dspy.OutputField(desc="Key themes and main characters identified in the story")

class StoryCreation(dspy.Signature):
    """Based on the key themes and moral of the story, create an entertaining story"""
    input_text: str = dspy.InputField()
    themes: str = dspy.InputField()
    moral: str = dspy.InputField()
    story: str = dspy.OutputField(desc="Create an entertaining story incorporating the themes and moral")



class StoryStructure(dspy.Module):
    def __init__(self):
        super().__init__()
        self.extract_relevant_information = dspy.ChainOfThought(ExtractRelevantInformation)
        self.story_creation = dspy.ChainOfThought(StoryCreation)

    def forward(self, input_text):
        # First, extract themes and moral from the initial text
        extracted_info = self.extract_relevant_information(
            input_text=input_text
        )
        
        # Then create the story using the extracted information
        story = self.story_creation(
            input_text=input_text,
            themes=extracted_info.key_themes,
            moral=extracted_info.moral_of_the_story
        )
        
        return {
            'themes': extracted_info.key_themes,
            'moral': extracted_info.moral_of_the_story,
            'story': story.story,
            'reasoning': extracted_info.reasoning
        }


# Initialize the story generator
generator = StoryStructure()

# Generate the story and return all components
result = generator(test_file)


result["story"]



def print_story_result(result_dict):
    print("\n" + "="*50 + " TEMAS " + "="*50)
    print(result_dict['themes'])
    
    print("\n" + "="*50 + " MORAL " + "="*50)
    print(result_dict['moral'])
    
    print("\n" + "="*50 + " HISTORIA " + "="*50)
    print(result_dict['story'])
    
    print("\n" + "="*50 + " RAZONAMIENTO " + "="*50)
    print(result_dict['reasoning'])
    print("\n" + "="*120)



# Print the result
print_story_result(result)


with open("new_story.txt", 'w', encoding='utf-8') as f:
        f.write(result["story"])


import torch
from transformers import AutoTokenizer
import soundfile as sf

from parler_tts import ParlerTTSForConditionalGeneration



device = "cuda:0" if torch.cuda.is_available() else "cpu"

model = ParlerTTSForConditionalGeneration.from_pretrained("parler-tts/parler-tts-mini-v1").to(device)
tokenizer = AutoTokenizer.from_pretrained("parler-tts/parler-tts-mini-v1")




prompt = result["story"]
description = "A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up."




input_ids = tokenizer(description, return_tensors="pt").input_ids.to(device)
prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)




generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)
audio_arr = generation.cpu().numpy().squeeze()



sf.write("parler_tts_out.wav", audio_arr, model.config.sampling_rate)
