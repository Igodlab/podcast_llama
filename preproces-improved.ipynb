{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# This notebook is a improved version of the PDF pre-processing of llama notebook\n",
    "\n",
    "In this notbook we implement our knowlegde on CoT, RAG and agentic systems for the procesing infomation o the documents.\n",
    "\n",
    "* Use a PDF (for now only text)\n",
    "* Use `groq-llama-3.2-3b-preview` for the proces of the file\n",
    "* Re write the PDF information on to a .txt for latter processing.\n",
    "\n",
    "For now the first test will be on the `Attention is all you need paper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "from groq import Groq\n",
    "from groq import Client\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "# from llama_index.llms.groq import Groq\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the pdf_path the name of the model we will use and declare the global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = \"/home/mau/Documents/Projects/podcast_llama/pdf_docs/Scaling Laws for Neural Language Modelspdf.pdf\"\n",
    "model_1 = \"llama-3.2-3b-preview\"\n",
    "\n",
    "api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now use llamaindex for parsing and extracting the document information. \n",
    "We have 2 ways of calling the model, \n",
    "1. Directly with groq:\n",
    "```python\n",
    "            from groq import Groq\n",
    "\n",
    "            client = Groq(\n",
    "                api_key=api_key,\n",
    "                model=model_1,\n",
    "            )\n",
    "```\n",
    "\n",
    "2. Using llamaidex \n",
    "```python\n",
    "            from llama_index.llms.groq import Groq\n",
    "            \n",
    "            llm = Groq(model=model_1, api_key=api_key)  \n",
    "```\n",
    "\n",
    "3. Using Client from groq-> `This is what we are going to implement`\n",
    "```python\n",
    "            from groq import Client\n",
    "\n",
    "            client = Client(\n",
    "                api_key=api_key\n",
    "                )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part is exactly the same as in the original notebook llama, this is to make sure the file exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_pdf(doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's do the retrieval of the information. \n",
    "\n",
    "We can use the `llamaindex` framework for this purpose. We will retrieve the information of the document, it can contain text, latex or tables and we want to read it all. \n",
    "\n",
    "The main differences with this proposition and metaAI is that we are using a LLM provider and a framework, we believe that this approach is more reliable and efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Optional\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path: str, max_chars: int ) -> Optional[Tuple[Dict, str]]:\n",
    "    \"\"\"\n",
    "    Process a PDF file to extract metadata and content with page tracking.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file\n",
    "        max_chars: Maximum number of characters to extract (default: 100000)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing metadata dictionary and extracted text, or None if validation fails\n",
    "    \"\"\"\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(\"\\nExtracting metadata...\")\n",
    "        \n",
    "        try:\n",
    "            pdf = PdfReader(file_path)\n",
    "            if not pdf.pages:\n",
    "                print(\"Error: PDF appears to be empty or corrupted\")\n",
    "                return None\n",
    "            total_pages = len(pdf.pages)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF with pypdf: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        doc_info = {\n",
    "            \"Pages\": total_pages,\n",
    "            \"Metadata\": pdf.metadata\n",
    "        }\n",
    "        \n",
    "        print(\"PDF Metadata:\")\n",
    "        print(f\"Number of pages: {total_pages}\")\n",
    "        print(\"Document info:\", doc_info['Metadata'])\n",
    "        \n",
    "        print(\"\\nExtracting text...\")\n",
    "        print(f\"Starting PDF processing (limit: {max_chars} characters)...\")\n",
    "        \n",
    "        try:\n",
    "            current_length = 0\n",
    "            text_content = \"\"\n",
    "            pages_processed = 0\n",
    "            \n",
    "            for page_num in range(total_pages):\n",
    "                if current_length >= max_chars:\n",
    "                    break\n",
    "                    \n",
    "                current_page = pdf.pages[page_num]\n",
    "                page_text = current_page.extract_text()\n",
    "                \n",
    "                remaining_chars = max_chars - current_length\n",
    "                if len(page_text) <= remaining_chars:\n",
    "                    text_content += page_text\n",
    "                    current_length += len(page_text)\n",
    "                    pages_processed += 1\n",
    "                    print(f\"Processed page {page_num + 1}/{total_pages} (Characters: {current_length}/{max_chars})\")\n",
    "                else:\n",
    "                    text_content += page_text[:remaining_chars]\n",
    "                    current_length += remaining_chars\n",
    "                    pages_processed += 1\n",
    "                    print(f\"Processed page {page_num + 1}/{total_pages} (Reached character limit of {max_chars})\")\n",
    "                    break\n",
    "            \n",
    "            # Save extracted text to file\n",
    "            output_file = \"extracted_text.txt\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "            print(f\"\\nProcessing complete:\")\n",
    "            print(f\"- Pages processed: {pages_processed} out of {total_pages}\")\n",
    "            print(f\"- Characters extracted: {current_length}\")\n",
    "            if pages_processed < total_pages:\n",
    "                print(f\"- Stopped at page {pages_processed} due to character limit\")\n",
    "            print(f\"- Extracted text saved to {output_file}\")\n",
    "            \n",
    "            return doc_info, text_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing PDF: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting metadata...\n",
      "PDF Metadata:\n",
      "Number of pages: 30\n",
      "Document info: {'/Author': '', '/CreationDate': 'D:20200124010539Z', '/Creator': 'LaTeX with hyperref package', '/Keywords': '', '/ModDate': 'D:20200124010539Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', '/Producer': 'pdfTeX-1.40.17', '/Subject': '', '/Title': '', '/Trapped': '/False'}\n",
      "\n",
      "Extracting text...\n",
      "Starting PDF processing (limit: 8000 characters)...\n",
      "Processed page 1/30 (Characters: 1647/8000)\n",
      "Processed page 2/30 (Characters: 3531/8000)\n",
      "Processed page 3/30 (Characters: 7089/8000)\n",
      "Processed page 4/30 (Reached character limit of 8000)\n",
      "\n",
      "Processing complete:\n",
      "- Pages processed: 4 out of 30\n",
      "- Characters extracted: 8000\n",
      "- Stopped at page 4 due to character limit\n",
      "- Extracted text saved to extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "result = process_pdf(doc_path, max_chars=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANING_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPITALIZATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_chunks(file_path: str, chunk_size: int = 16):\n",
    "    \"\"\"\n",
    "    Load text, chunk it, and clean each chunk using Groq\n",
    "    \"\"\"\n",
    "    # Load the document\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    \n",
    "    # Initialize text splitter\n",
    "    text_splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=4)\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(documents[0].text)\n",
    "    print(f\"Split text into {len(chunks)} chunks\")\n",
    "    \n",
    "     # Clean each chunk\n",
    "    cleaned_text = \"\"\n",
    "    for chunk in tqdm(chunks, desc=\"Cleaning chunks\"):\n",
    "        try:\n",
    "            # Use Groq's chat completions\n",
    "            completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": CLEANING_PROMPT\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": chunk\n",
    "                    }\n",
    "                ],\n",
    "                model=model_1,\n",
    "                temperature=0.7,\n",
    "                max_tokens=1024\n",
    "            )\n",
    "            \n",
    "            # Extract cleaned text\n",
    "            cleaned_chunk = completion.choices[0].message.content\n",
    "            cleaned_text += cleaned_chunk + \"\\n\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "    # Save cleaned text\n",
    "    output_file = \"cleaned_text.txt\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_text)\n",
    "    \n",
    "    print(f\"\\nCleaned text saved to {output_file}\")\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 157 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939a2a4954c24355b6a2fd1c905a96bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning chunks:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned text saved to cleaned_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Use the function\n",
    "cleaned_text = clean_text_chunks(\"/home/mauricio/Documents/Projects/podcast_llama/extracted_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_podcastllama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
